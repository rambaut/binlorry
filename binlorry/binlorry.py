"""
Copyright 2019 Andrew Rambaut (a.rambaut@ed.ac.uk) & √Åine O'Toole
https://github.com/rambaut/Binlorry

This module contains the main script for BinLorry. It is executed when a user runs `binlorry`
(after installation) or `binlorry-runner.py` (directly from the source directory).

This file is part of BinLorry. BinLorry is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by the Free Software Foundation,
either version 3 of the License, or (at your option) any later version. BinLorry is distributed in
the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details. You should have received a copy of the GNU General Public License along with BinLorry. If
not, see <http://www.gnu.org/licenses/>.
"""

import argparse
import csv
import gzip
import os
import sys

from .misc import bold_underline, MyHelpFormatter, get_sequence_file_type, get_compression_type, get_file_stem
from .version import __version__


def main():
    '''
    Entry point for BinLorry. Gets arguments, processes them and then calls process_files function
    to do the actual work.
    :return:
    '''
    args = get_arguments()

    filters = []
    if args.filter_by:
        for filter_def in args.filter_by:
            values = filter_def[1:]
            filters.append({ 'field': filter_def[0], 'values': values})

    if args.verbosity > 0:
        print(bold_underline("\nBins and filters:"),flush=True, file=args.print_dest)
        for filter in filters:
            print("Filter reads unless " + filter['field'] + (" is one of: " if len(filter['values']) > 1 else " is "), end = '')
            for index, value in enumerate(filter['values']):
                print((", " if index > 0 else "") + value, end = '')
            print()

        if args.min_length and args.max_length:
            print("Filter reads unless length between " + str(args.min_length) + " and " + str(args.max_length))
        elif args.min_length:
            print("Filter reads unless length at least " + str(args.min_length))
        elif args.max_length:
            print("Filter reads unless length not more than " + str(args.max_length))

        if args.bin_by:
            print("Bin reads by ", end = '')
            for index, bin in enumerate(args.bin_by):
                print((", " if index > 0 else "") + bin, end = '')
            print("")

        if args.header_delimiters:
            print("Header delimiters ", args.header_delimiters)

    process_files(args.input_path, args.data_path, args.unordered_data,
                  args.bin_by, filters,
                  getattr(args, 'min_length', 0), getattr(args, 'max_length', 1E10),
                  getattr(args, 'header_delimiters', "="),
                  args.output, args.force_output,
                  args.verbosity, args.print_dest, args.out_report)

# def read_index_table(index_table_file):
#     unfiltered_index_table = pd.read_csv(index_table_file)
#     headers = unfiltered_index_table.columns.values
#     return unfiltered_index_table, headers

# def filter_index_table(unfiltered_index_table, filters, print_dest):
#     # For each filter specified in args, filter the pd dataframe to only include the values given.
#     # In case of key error (i.e. filter in cmd line arg not present as a header in the csv) print informative error.
#     for filter in filters:
#         try:
#             filtered_index_table = unfiltered_index_table.loc[(unfiltered_index_table[filter["field"]].isin(filter["values"]))]
#         except:
#             print("Check if csv has '{}' column.".format(filter["field"]))
#     return filtered_index_table

def process_files(input_path, input_report_path, unordered_data,
                  bins, filters, min_length, max_length, header_delimiters, output_prefix,
                  force_output, verbosity, print_dest, create_output_report):
    """
    Core function to process one or more input files and create the required output files.

    Iterates through the reads in one or more input files and bins or filters them into the
    output files as required.
    """

    read_files = get_input_files(input_path, verbosity, print_dest)

    if verbosity > 0:
        print(bold_underline('\nRead files found:'), flush=True, file=print_dest)
        for read_file in read_files:
            print(read_file, flush=True, file=print_dest)

    # A dictionary to store output files. These are created as needed when the first
    # read in each bin is being written. The prefix and suffix of the filenames are stored
    # here for convenience.
    output_files = {
        'prefix': output_prefix,
        'suffix': '.fastq' # todo - this should be fasta if appropriate
    }

    output_reports = None
    if create_output_report:
        output_reports = {
            'prefix': output_prefix,
            'suffix': '.csv'
        }

    if force_output:
        if len(bins) > 1:
            print(bold_underline("\nWarning: --force_output only works on a single bin/filter factor."))
            print(bold_underline("Usage: --force_output requires --bin_by to have matching --filter_by."))
        else:
            for filter in filters:
                for bin in bins:
                    if filter["field"] == bin:
                        print(bold_underline("\nForcing file creation of: "))
                        for value in filter["values"]:
                            filename = output_prefix + "_" + value + ".fastq"
                            with open(filename, "w"):
                                print(filename)
                                output_files[bin] = filename

                            if create_output_report:
                                report_name = filename.rstrip("fastq")+("csv")
                                with open(report_name, "w"):
                                    print(report_name)
                                    output_reports[bin] = report_name

    counts = { 'read': 0, 'passed': 0, 'bins': {} }

    report_dict = None
    data_table = None

    if input_report_path:
        report_dict = get_input_reports(input_report_path, verbosity, print_dest)

        # if '--unordered_data' is set then all the data tables should be loaded to be accessed as needed
        if unordered_data:
            data_table = {}
            for key in report_dict:
                data_table = read_data_table(report_dict[key], data_table)


    if verbosity > 0:
        print('\n' + bold_underline('Processing read file:'), flush=True, file=print_dest)

    for read_file in read_files:
        print(read_file, flush=True, file=print_dest)

        report_file = None

        if report_dict:
            # is there a matching report file?
            file_stem, suffix = get_file_stem(read_file, [".fasta", ".fasta.gz", ".fastq", ".fastq.gz"])
            if file_stem not in report_dict:
                continue # If fasta file is created but not demultiplexed yet can get errors otherwise
            report_file = report_dict[file_stem]

        # this can be called with a report_file containing the data for this read_file, a data_table which
        # is a dictionary of all report files keyed by read name or neither (in which case the data is parsed
        # from the read headers)
        process_read_file(read_file, report_file, data_table, bins, filters, min_length, max_length,
                          header_delimiters, output_files, output_reports, counts, verbosity, print_dest)

    if output_files:
        for file in output_files:
            if hasattr(file, 'close'):
                file.close()

    if output_reports:
        for file in output_reports:
            if hasattr(file, 'close'):
                file.close()

    if verbosity > 0:
        print(bold_underline('\nSummary:')+'\nTotal reads read: ' + str(counts['read']), file=print_dest)
        print('Total reads written: ' + str(counts['passed']), file=print_dest)
        print(bold_underline('\nRead files written:'), file=print_dest)
        for file in counts['bins']:
            if output_reports:
                print(file + " with corresponding csv report file.", file=print_dest)
            else:
                print(file, file=print_dest)
        print("\n", file=print_dest)


def get_input_files(input_path, verbosity, print_dest):
    '''
    Takes a path to a single file or a directory and returns a list of file paths to be processed.
    :param input_file_or_directory: The input path
    :param verbosity: Verbosity level to report
    :param print_dest: Where to report (stdout or stderr)
    :return: An array of file paths to process
    '''
    input_files = []

    if os.path.isfile(input_path):
        input_files.append(input_path)

    # If the input is a directory, search it recursively for fastq files.
    elif os.path.isdir(input_path):
        input_files = sorted([os.path.join(dir_path, f)
                              for dir_path, _, filenames in os.walk(input_path)
                              for f in filenames
                              if f.lower().endswith('.fastq') or f.lower().endswith('.fastq.gz') or
                              f.lower().endswith('.fasta') or f.lower().endswith('.fasta.gz')])
        if not input_files:
            sys.exit('Error: could not find FASTQ/FASTA files in ' + input_path)

    else:
        sys.exit('Error: could not find ' + input_path)

    return input_files


def get_input_reports(input_report_path, verbosity, print_dest):
    '''
    returns a report dict with read file as key and report file as value (incl. paths).
    :param input_report_path: The path to the report files
    :param verbosity: Verbosity level to report
    :param print_dest: Where to report (stdout or stderr)
    :return: An dictionary of report files
    '''

    input_report_files = []

    if os.path.isfile(input_report_path):
        input_report_files.append(input_report_path)

    # If the input is a directory, search it recursively for fastq files.
    elif os.path.isdir(input_report_path):
        input_report_files = sorted([os.path.join(dir_path, f)
                                     for dir_path, _, filenames in os.walk(input_report_path)
                                     for f in filenames
                                     if f.lower().endswith(".csv")])

        if not input_report_files:
            sys.exit('Error: could not find CSV in ' + input_report_path)

    else:
        sys.exit('Error: could not find ' + input_report_path)

    report_dict = {}
    for file_name in input_report_files:
        file_stem, suffix = get_file_stem(file_name, [".csv"])
        if not suffix.lower().endswith(".csv"):
            sys.exit('Error: ' + file_name + ' not a .csv file')

        report_dict[file_stem] = file_name

    if verbosity > 0:
        print('\n' + bold_underline('Report files found:'), flush=True, file=print_dest)
        for i in sorted(report_dict):
            print(report_dict[i], flush=True, file=print_dest)

    return report_dict


def process_read_file(read_file, report_file, data_table,
                      bins, filters, min_length, max_length, header_delimiters,
                      output_files, output_reports, counts, verbosity, print_dest):
    """
    Iterates through the reads in an input files and bins or filters them into the
    output files as required.
    """

    file_type = get_sequence_file_type(read_file)

    if get_compression_type(read_file) == 'gz':
        open_func = gzip.open
    else:  # plain text
        open_func = open

    report_fields = None

    if report_file:
        data_table = read_data_table(report_file, {})
        report_fields = get_report_fields(data_table)

    line_number = 0

    with open_func(read_file, 'rt') as in_file:

        if file_type == 'FASTA':
            # For FASTA files we need to deal with line wrapped sequences...

            header = ''
            sequence = ''

            for line in in_file:
                line = line.strip()

                if not line:
                    continue

                if line[0] == '>':  # Header line = start of new read
                    if header:
                        line_number += 1
                        data = get_read_data(header, header_delimiters, data_table)
                        filter_bin_read(output_files, output_reports, filters, bins, min_length, max_length, data, header, sequence, None, counts, print_dest)
                        sequence = ''
                    header = line[1:]
                else:
                    sequence += line

            if header:
                line_number += 1
                data = get_read_data(header, header_delimiters, data_table)
                filter_bin_read(output_files, output_reports, filters, bins, min_length, max_length, data, header, sequence, None, counts, print_dest)

        else: # FASTQ
            for line in in_file:
                header = line.strip()[1:]
                sequence = next(in_file).strip()
                next(in_file) # spacer line
                qualities = next(in_file).strip()

                line_number += 1
                data, name = get_read_data(header, header_delimiters, data_table)
                filter_bin_read(output_files, output_reports, filters, bins, min_length, max_length, data, header, sequence, qualities, counts, print_dest)

        in_file.close()

    if output_files:
        for file in output_files:
            if hasattr(file, 'close'):
                file.close()


def get_read_data(header, header_delimiters, data_table):

    name = header.split(' ')[0]

    if data_table:
        if not name in data_table:
            sys.exit('Error: read with name, ' + name + ', is missing from data table')

        return data_table[name], name

    else:
        return get_header_data(header, header_delimiters), name


# def get_report_data(file, line, line_number, name, report_fields):
#     data = {}
#     values = line.split(',')
#
#     if len(values) != len(report_fields):
#         sys.exit('input report file, ' + file.name + ', line ' + str(line_number + 1) + ', has the wrong number of fields')
#
#     if name and values[0] != name:
#         sys.exit("input report file, " + file.name + ", line " + str(line_number + 1) +
#                  ", the first column doesn't match the name of the read")
#
#     for i in range(len(values)):
#         data[report_fields[i]] = values[i]
#
#     return data, values[0]

def read_data_table(data_path, data_table):

    # get the field names for the existing data_table
    fields = get_report_fields(data_table)

    with open(data_path, 'rt') as csvfile:
        new_data_table = csv.DictReader(csvfile)

        line_number = 0

        for row in new_data_table:
            if line_number == 0:
                # check this data table has the same fields as the existing one
                for key in fields:
                    if key not in row:
                        sys.exit('Error: input report file, ' + csvfile.name + ', is missing column ' + key)

                if "read_name" not in row:
                    sys.exit('Error: input report file, ' + csvfile.name + ', is missing column "read_name"')

            read_name = row["read_name"]

            if read_name in data_table:
                sys.exit('Error: duplicate read name, "' + read_name + '", input report file, ' + csvfile.name + ', line ' + str(line_number + 1))

            # add the data to the existing data table
            data_table[read_name] = row

            line_number += 1
            
    # print(bold_underline('\nHeaders:'), flush=True, file=print_dest)
    # print("\n".join(headers),flush=True, file=print_dest)

    return data_table


def get_report_fields(data_table):
    if not data_table or len(data_table.keys()) == 0:
        return []

    return data_table[next(iter(data_table))].keys()


def filter_bin_read(output_files, output_reports, filters, bins, min_length, max_length, data,
                    header, sequence, qualities, counts, print_dest):
    '''
    Writes a read in either FASTQ or FASTA format depending if qualities are given
    :param out_file: The file to write to
    :param header:  The header line
    :param sequence: The sequence string
    :param qualities: The qualities string (None if FASTA)
    '''
    counts['read'] += 1

    length = len(sequence)

    if length > min_length and length < max_length:

        passed = read_passes_filters(data, filters)

        if passed:
            counts['passed'] += 1
            out_file = get_bin_output_file(data, bins, output_files, False)

            out_report = None
            if output_reports:
                out_report = get_bin_output_file(data, bins, output_reports, True)
                pass
            # todo - create output reports
            # col_names = ",".join(list(filtered_index_table.columns.values))
            # out_report = get_bin_output_file(data, bins, out_reports, col_names, True)
            #
            # row = filtered_index_table.loc[filtered_index_table["read_name"] == fields["read_name"]].values
            # row_str = ",".join([str(i) for i in list(row[0])])
            # out_report.write(row_str + "\n")

            if out_file:
                write_read(out_file, header, sequence, qualities)

                if out_report:
                    out_report.write(",".join(data.values()) + "\n")

                if not out_file.name in counts['bins']:
                    counts['bins'][out_file.name] = 0
                counts['bins'][out_file.name] += 1


def write_read(out_file, header, sequence, qualities):
    '''
    Writes a read in either FASTQ or FASTA format depending if qualities are given
    :param out_file: The file to write to
    :param header:  The header line
    :param sequence: The sequence string
    :param qualities: The qualities string (None if FASTA)
    '''

    if qualities: # FASTQ
        read_str = ''.join(['@', header, '\n', sequence, '\n+\n', qualities, '\n'])
    else:         # FASTA
        read_str = ''.join(['>', header, '\n', sequence, '\n'])

    out_file.write(read_str)


def read_passes_filters(header_fields, filters):
    '''
    Returns true if the read passes all the filters
    :param fields:
    :param header_filters:
    :return:
    '''
    for filter in filters:

        if filter['field'] in header_fields:
            if not header_fields[filter['field']] in filter['values']:
                return False
        else:
            return False

    return True


def  get_bin_output_file(fields, bins, output_files, report):
    '''
    This function decides which file to send this read to based on the current bins and filters. If
    the read's fields do not pass the filters then None is returned. Otherwise the file that is associated
    with the appropriate bin is returned. This will be created if it hasn't been already and the handle
    stored in `out_files`.

    :return: the appropriate output file if passes the filter, None if not
    # '''

    if bins:
        bin_name = ""

        for bin in bins:

            if bin in fields:
                bin_name += "_" + fields[bin]

        if len(bin_name) > 0:
            if not bin_name in output_files:
                output_files[bin_name] = open(output_files['prefix'] + bin_name + output_files['suffix'], "wt")
                if report:
                    output_files[bin_name].write(",".join(fields) +'\n')
            return output_files[bin_name]

    if not 'unbinned' in output_files:
        output_files['unbinned'] = open(output_files['prefix'] + output_files['suffix'], "wt")
        if report:
            output_files["unbinned"].write(",".join(fields) +'\n')
    return output_files['unbinned']


def get_csv_fields(index_table,header):
    fields = {}
    name=header.split(' ')[0]
    try:
        row=index_table.query('read_name=="{}"'.format(name))
        fields = {'name': name}
        for i in row:
            fields[i]=list(row[i])[0]
    except:
        pass


    return fields

def get_header_data(header, header_delimiters):
    '''
    Splits a FASTA/FASTQ header line into key=value fields and returns them as
    an object.
    :param header:
    :return: An object of key value pairs
    '''

    parts = header.split(' ')

    fields = { 'name': parts[0] }

    for part in parts[1:]:
        for delimiter in header_delimiters:
            try:
                (key, value) = part.split(delimiter)
                fields[key] = value
            except:
                pass

    return fields

def get_arguments():
    '''
    Parse the command line arguments.
    '''
    parser = argparse.ArgumentParser(description='BinLorry: a tool for binning sequencing reads into '
                                                 'files based on header information or read properties.',
                                     formatter_class=MyHelpFormatter, add_help=False)

    main_group = parser.add_argument_group('Main options')
    main_group.add_argument('-i', '--input', dest='input_path', required=True,
                            help='FASTA/FASTQ of input reads or a directory which will be '
                                 'recursively searched for FASTQ files (required).')
    main_group.add_argument('-t', '--data', metavar='CSV_FILE', dest='data_path', default=None,
                            help='A CSV file with metadata fields for reads or a directory of csv files that will '
                                 'be recursively searched for names corresponding to a matching input FASTA/FASTQ '
                                 'files.')
    main_group.add_argument('-u', '--unordered_data', action='store_true',
                            help='The metadata tables are not in the same order as the reads - they will all be'
                                 'loaded and then looked up as needed (slower).')
    main_group.add_argument('-o', '--output', required=True,
                            help='Output filename (or filename prefix)')
    main_group.add_argument('-r', '--out-report',action='store_true',dest="out_report",
                            help='Output a report along with FASTA/FASTQ.')
    main_group.add_argument('-f', '--force-output',action='store_true',dest="force_output",
                            help='Output binned/filtered files even if empty.')
    main_group.add_argument('-v', '--verbosity', type=int, default=1,
                            help='Level of output information: 0 = none, 1 = some, 2 = lots')

    bin_group = parser.add_argument_group('Binning/Filtering options')
    bin_group.add_argument('--bin-by', metavar='FIELD', nargs='+', dest='bin_by',
                           help='Specify header field(s) to bin the reads by. For multiple fields these '
                                'will be nested in order specified. e.g. `--bin-by barcode reference`')
    bin_group.add_argument('--filter-by', metavar='FILTER', action='append', nargs='+', dest='filter_by',
                           help='Specify header field and accepted values to filter the reads by. Multiple '
                                'instances of this option can be specified. e.g. `--filter-by barcode BC01 BC02'
                                '--filter-by genotype Type1`')
    bin_group.add_argument('-n', '--min-length', metavar='MIN', type=int, dest='min_length',
                           help='Filter the reads by their length, specifying the minimum length.')
    bin_group.add_argument('-x', '--max-length', metavar='MAX', type=int, dest='max_length',
                           help='Filter the reads by their length, specifying the maximum length.')
    bin_group.add_argument('-d', '--header-delimiters', metavar='DELIM', dest='header_delimiters', default="=",
                           help='Delimiters to use when searching for key:value pairs in FASTA/FASTQ header.')


    help_args = parser.add_argument_group('Help')
    help_args.add_argument('-h', '--help', action='help', default=argparse.SUPPRESS,
                           help='Show this help message and exit')
    help_args.add_argument('--version', action='version', version=__version__,
                           help="Show program's version number and exit")

    args = parser.parse_args()

    if args.output is None:
        # output is to stdout so print messages to stderr
        args.print_dest = sys.stderr
    else:
        args.print_dest = sys.stdout
        os.makedirs(os.path.dirname(args.output), exist_ok=True)

    return args


